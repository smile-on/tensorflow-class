{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation at runtime\n",
    "Using ImageDataGenerator from [keras image preprocessing](https://keras.io/preprocessing/image/#image-preprocessing). Watch out overfeatting on lerning history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    # instead of  rescale=1./255\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    # randomly select transformation within given limits in %\n",
    "    rotation_range=20,     # 20%\n",
    "    width_shift_range=0.2, # 20%\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nerest'\n",
    ")\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit example and augmentation n action [Cats vs Dogs](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%204%20-%20Lesson%202%20-%20Notebook%20(Cats%20v%20Dogs%20Augmentation).ipynb)\n",
    "\n",
    "Note, you need image **variations in training input data** and similary variations needs to be in your **test/validation set as well**, otherwise validation score would flactuate dramatically and not represent actual level of confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "Example in colab [Transfer Learning.ipynb](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%206%20-%20Lesson%203%20-%20Notebook.ipynb) same as used in tutorial [Transfer Learning on tensorflow.org](https://www.tensorflow.org/tutorials/images/transfer_learning).\n",
    "\n",
    "create the base model from [prebuilt repo in keras](https://keras.io/applications/#applications): \n",
    "Xception\n",
    "VGG16\n",
    "VGG19\n",
    "ResNet, ResNetV2\n",
    "InceptionV3\n",
    "InceptionResNetV2\n",
    "MobileNet\n",
    "MobileNetV2\n",
    "DenseNet\n",
    "NASNet\n",
    ".\n",
    "\n",
    "**Transfer base model** for feature extraction \n",
    "```python\n",
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet' # use built in weights\n",
    "                                              )\n",
    "# Freeze the Feature extraction convolutional base before we compile and train the model. \n",
    "base_model.trainable = False\n",
    "\n",
    "# Add a classification head\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  keras.layers.GlobalAveragePooling2D(),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile()\n",
    "history = model.fit_generator(train_generator, ..)\n",
    "```\n",
    "\n",
    "* Weights can be saved/loaded localy as snapshot\n",
    "```python\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None\n",
    "                                              )\n",
    "base_model.load_weights('/tmp/kernels_v123.h5')\n",
    "```\n",
    "\n",
    "\n",
    "* Some models may require special pre-processing and post-processing\n",
    "```python\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "```\n",
    "\n",
    "**Fine tuning layers** You can start fine tune few top layers after base model trained to converge on your data.\n",
    "```python\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "# alternative reference by name\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "```\n",
    "\n",
    "* Note, the room for fine tuning may be limited by overfitting\n",
    "![fine-tune-loss.png](fine-tune-loss.png)\n",
    "\n",
    "* Don't forget add Dropout layers into your classifier to fight overfitting. [Dropout regularization note by Ng](https://www.youtube.com/watch?v=ARq74QuavAo). Seting dropout level too high may result in NN would lose specialization to the effect that it would be inefficient or ineffective at learning, driving accuracy down.\n",
    "\n",
    "**NASNet** paper [2017 Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
