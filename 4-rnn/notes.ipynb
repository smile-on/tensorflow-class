{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN class of NN\n",
    "Ng course on **sequence modeling** [ @Coursera](https://www.coursera.org/lecture/nlp-sequence-models/deep-rnns-ehs0S) or [@Youtube](https://youtu.be/DejHQYAGb7Q?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6). Special note Ng's lectures on Bidirectional RNN as **attention model** @youtube [C5W3L07](https://youtu.be/SysgYptB198) [C5W3L08](https://youtu.be/quoGRI-1l0A)\n",
    "\n",
    "LSTN in pictures :) [@Coursera](https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay) No flattenng or pooling needed, LSTM input = *sequence* but output is not (by default), use `return_sequences` to pass whole sequence as output.\n",
    "\n",
    "```python\n",
    "    keras.layers.Embending(tokenizer.vocab_size, 42),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(42, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(12)),\n",
    "    keras.layers.Dense(8),\n",
    "```\n",
    "Note, in some cases Conv1D can be used instead of RNN to make feature extractor filters from sequence.\n",
    "Runtime of training LSTM twice slover x2 GRU or x10 Dense of same size. Training time CNN ~ Dense due to hw acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 33, 42)            5040      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 33, 112)           44352     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 24)                12000     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 200       \n",
      "=================================================================\n",
      "Total params: 61,592\n",
      "Trainable params: 61,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model =  keras.models.Sequential([\n",
    "    keras.layers.Embedding(120, 42, input_shape=(33,)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(56, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(12)),\n",
    "    keras.layers.Dense(8),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 33, 42)            5040      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 29, 56)            11816     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 7, 56)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 24)                6624      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 200       \n",
      "=================================================================\n",
      "Total params: 23,680\n",
      "Trainable params: 23,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model =  keras.models.Sequential([\n",
    "    keras.layers.Embedding(120, 42, input_shape=(33,)),\n",
    "    keras.layers.Conv1D(56, 5, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(12)),\n",
    "    keras.layers.Dense(8),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, The sequence is a unit of input for [RNN learning](https://keras.io/getting-started/sequential-model-guide/#training)!\n",
    "**RNN** learns **probability for whole sequence** not for internal relations inside.\n",
    "\n",
    "Here is example. Fun application, text prediction by RNN -> new context generator! AI can \"write\" poems, \"news\" etc. In order for RNN to learn probability in N-gram, we need to split each sentence in n-gram sequences that feeds RNN word by word. That forces learning probability for each n-gram not just \"whole\" sentence. Predict next word => multi-categorical classification problem as each word is a unique label not a number in continius Rn space.\n",
    "```\n",
    " sentence = \"How are you today?\"\n",
    " encoded = [7 3 4 9]\n",
    " \n",
    " # keras.preprocessing.sequence.skipgrams()\n",
    " n_grams_padded = [\n",
    "  [0 0 0 7 3],\n",
    "  [0 0 7 3 4],\n",
    "  [0 7 3 4 9]\n",
    " ]\n",
    " \n",
    " xs = [\n",
    "  [0 0 0 7],\n",
    "  [0 0 7 3],\n",
    "  [0 7 3 4]\n",
    " ]\n",
    " labels = [\n",
    "  3,\n",
    "  4,\n",
    "  9\n",
    " ]\n",
    "\n",
    "# one-hot encoding into \"word\" categories!\n",
    "ys = keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "```\n",
    "Examples\n",
    "- by Laurence [LSTM Shakespeare](https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP_Week4_Exercise_Shakespeare_Answer.ipynb)\n",
    "- TF example [generating text using a character-based RNN](https://www.tensorflow.org/tutorials/text/text_generation).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
