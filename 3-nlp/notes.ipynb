{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoding\n",
    "## Encoding text word-by-word based on frequency\n",
    "\n",
    "* [Tokenizer](https://keras.io/preprocessing/text/#tokenizer) by word frequency with some minimal lexical pre-processing **texts_to_sequences()**\n",
    "* [pad_sequences](https://keras.io/preprocessing/sequence/#pad_sequences) up to expected NN input size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'you': 2, 'are': 3, 'love': 4, 'do': 5, 'owe': 6, 'need': 7, 'a': 8, 'break': 9, \"your's\": 10, 'jokes': 11, 'the': 12, 'best': 13, 'so': 14, 'butefull': 15, 'to': 16, 'me': 17}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer example\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'I love you!',\n",
    "    'do i owe you?'\n",
    "    'i need a break',\n",
    "    'your\\'s jokes are the BEST.' ,\n",
    "    'you are so butefull to me'\n",
    "]\n",
    "\n",
    "tknzr = Tokenizer(num_words=100) # top 100\n",
    "tknzr.fit_on_texts(sentences)\n",
    "idx = tknzr.word_index\n",
    "print(idx)\n",
    "# note no punctuations or upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 2], [5, 1, 6, 2, 1, 7, 8, 9], [10, 11, 3, 12, 13], [2, 3, 14, 15, 16, 17]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tknzr.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love you!', 'i REALLY love you!']\n",
      "[[1, 4, 2], [1, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "# ignoring unknown words by default, safe for inference with trained NN\n",
    "test_data = ['i love you!' , 'i REALLY love you!']\n",
    "print(test_data)\n",
    "print(tknzr.texts_to_sequences(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love you!', 'i REALLY love you!']\n",
      "[[2, 5, 3], [2, 1, 5, 3]]\n",
      "{'<OOV>': 1, 'i': 2, 'you': 3, 'are': 4, 'love': 5, 'do': 6, 'owe': 7, 'need': 8, 'a': 9, 'break': 10, \"your's\": 11, 'jokes': 12, 'the': 13, 'best': 14, 'so': 15, 'butefull': 16, 'to': 17, 'me': 18}\n"
     ]
    }
   ],
   "source": [
    "# to capture difference in meaning with unknown words, use special token to encode unknown word\n",
    "tknzr2 = Tokenizer(num_words=100, oov_token='<OOV>') # Out Of Vacubulary data\n",
    "tknzr2.fit_on_texts(sentences)\n",
    "print(test_data)\n",
    "print(tknzr2.texts_to_sequences(test_data))\n",
    "print(tknzr2.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evry sentence after encoding should be uniform (same length) by padding up to expected NN input size.\n",
    "* [pad_sequences](https://keras.io/preprocessing/sequence/#pad_sequences) is available in keras.preprocessing.sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 5, 3], [2, 1, 5, 3]] <class 'list'> \n",
      "----\n",
      " [[0 2 5 3]\n",
      " [2 1 5 3]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "unpadded = tknzr2.texts_to_sequences(test_data)\n",
    "padded  = pad_sequences(unpadded) # default = pre \n",
    "print(unpadded, type(unpadded), '\\n----\\n', padded, type(padded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 2], [5, 1, 6, 2, 1, 7, 8, 9], [10, 11, 3, 12, 13], [2, 3, 14, 15, 16, 17]] \n",
      "---\n",
      " [[ 1  4  2  0  0]\n",
      " [ 5  1  6  2  1]\n",
      " [10 11  3 12 13]\n",
      " [ 2  3 14 15 16]]\n"
     ]
    }
   ],
   "source": [
    "# padding up to expected NN input size\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "print( sequences, '\\n---\\n', padded) # pre or post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Embedding takes encoded words and establish sentiment from them, so that you can begin to classify and then later predict texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
