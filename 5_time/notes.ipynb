{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series\n",
    "Time Series sequences and prediction of sizonal variations and trends in data by [Laurence Moroney with examples](https://github.com/lmoroney/dlaicourse/tree/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP).\n",
    "\n",
    "Time Series :\n",
    "- **Univariate** - single value at each step in time\n",
    "- **Multivariate** - multiple values at each step in time\n",
    "\n",
    "Used as search of patterns in time:\n",
    "- Prediction of future = forecast, past = imputation\n",
    "- Anomaly detection\n",
    "- Feature detection in speach recognition\n",
    "\n",
    "Common patterns in time series:\n",
    "- **trend** constant or linear function\n",
    "- **seasonality** when same patterns repeat at predictable intervals\n",
    "- **combination** of trend and seasonality\n",
    "- **autocorrelation** it correlates with a delayed copy of itself often called a lag.\n",
    "- **white noise** truly random signal can't be predicted.\n",
    "- **Non-stationary** different patterns within different time scope in data.\n",
    "\n",
    "Note **Non-stationary** time series may need few different models (each with limited time scope in data) + anomaly detection.\n",
    "\n",
    "![autocorrelation1](autocorrelation1.jpg)\n",
    "![autocorrelation2](autocorrelation2.jpg)\n",
    "![realmix](realmix.jpg)\n",
    "![non-stationary](non-stationary.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric in timeseries\n",
    "train, test data sets and error measure in TS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Partitions\n",
    "Note difference TS-ML with classical ML:\n",
    "- You whant to include **full cycles** in all sets to avoid bias of diferrent representation in data sets.\n",
    "- tune hyper parameters on validation set\n",
    "- include validation set in training and check final performance on test set\n",
    "- if fianl performance is good before releasing, train production model on whole data set including test, as test has most recent data and more relevant signal for the forecast.\n",
    "\n",
    "![fixed-partitions.jpg](fixed-partitions.jpg)\n",
    "\n",
    "### Rolling Partitions\n",
    "\n",
    "- take small portion of training period and try to predict next day (still inside training set)\n",
    "- add that day to training and try to predict next day\n",
    "- repeat until all training set is used. \n",
    "Monitor model performance on validation set. Model accuracy should gradualy increase.\n",
    "\n",
    "Sometime you don't allocate test set and use next day life data as your test set.\n",
    "\n",
    "![roll-partitions.jpg](roll-partitions.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for evaluating performance\n",
    "\n",
    "**MSE** (L2 metric) penalizes proportinal to error value, big errors affects function more severly.\n",
    "\n",
    "**MAE** (L1 metric) does not penalize large errors as much as the mse does. \n",
    "If your gain or your loss is just slightly proportional to the size of the error, then the mae may be better.\n",
    "\n",
    "**MAPE** - mean absolute percentage error, this is the mean ratio between the absolute error and the absolute value, this gives an idea of the size of the errors compared to the values.\n",
    "\n",
    "```python\n",
    "mae = keras.metrics.mean_absolute_error(y_validation, prediction).numpy()\n",
    "```\n",
    "\n",
    "![](err-metric.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base line (relative performance) : naive forecast, moving average (rolling average) or moving average of difference.\n",
    "\n",
    "\"constant\" naive forecast -> f(t+1) = f(t)\n",
    "\n",
    "![](relative-performance.jpg)\n",
    "\n",
    "Note, \n",
    "mae(naive forecast) = 5.7\n",
    "mae(moving average) = 7.14\n",
    ",  cause components: trend, seasonality (?). **differencing** is a technique to remove trend, seasonality. So instead of studying the time series itself, we study the difference between the value at time T and the value at an earlier period. Differencing requires **time lag** assumption.\n",
    "\n",
    "![](differencing1.jpg)\n",
    "\n",
    "base line prediction `f(t) = f(t-lag) + roll_avg(differencing[t, window])`\n",
    "with moving window = 32\n",
    "\n",
    "![](differencing2.jpg)\n",
    "\n",
    "smoth by average 2 with centered window = \\[t-360, t+5 \\]?! for prediction (validation test?) we can't use centered window as we dont know future, use trailing windows.\n",
    "\n",
    "![](differencing3.jpg)\n",
    "\n",
    "Note, \n",
    "mae(naive forecast) = 5.7\n",
    "mae(smooth moving average) = 4.5!\n",
    "\n",
    "statistical forecasting = use linear aproximations in TS (mainly smooth moving average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing features and labels\n",
    "Great support for Time Series in [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). A **Dataset** can be used to represent an input pipeline as a collection of elements and a \"logical plan\" of transformations that act on those elements.\n",
    "\n",
    "```python\n",
    "a = Dataset.range(1, 3)   # ==> [ 1, 2 ]\n",
    "b = Dataset.range(4, 8)   # ==> [ 4, 5, 6, 7 ]\n",
    "a.concatenate(b).batch(2) # ==> [[1,2], [ 4, 5], [6, 7]] \n",
    "d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorSliceDataset <> narray! how to inspect?\n",
    "d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2], dtype=int64), array([4, 5], dtype=int64), array([6, 7], dtype=int64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 3)  \n",
    "b = tf.data.Dataset.range(4, 8)  \n",
    "ds = a.concatenate(b).batch(2)   # BatchDataset([tf.Tensor<tf.int64>])\n",
    "print([tensors.numpy() for tensors in ds])  # .numpy() .shape .dtype\n",
    "ds == [[1,2], [4,5], [6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 \n",
      "1 2 3 \n",
      "2 3 4 \n",
      "3 4 5 \n",
      "4 5 6 \n",
      "5 6 7 \n",
      "6 7 8 \n",
      "7 8 9 \n",
      "8 9 \n",
      "9 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "raw_ds = tf.data.Dataset.range(10)\n",
    "rolling_ds = raw_ds.window(3, shift=1)\n",
    "for window_ds in rolling_ds:\n",
    "    for val in window_ds:\n",
    "        print(val.numpy(), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 \n",
      "1 2 3 \n",
      "2 3 4 \n",
      "3 4 5 \n",
      "4 5 6 \n",
      "5 6 7 \n",
      "6 7 8 \n",
      "7 8 9 \n"
     ]
    }
   ],
   "source": [
    "# drop_remainder - cut full windows only\n",
    "rolling_ds = raw_ds.window(3, shift=1, drop_remainder=True)\n",
    "for window_ds in rolling_ds:\n",
    "    for val in window_ds:\n",
    "        print(val.numpy(), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[1 2 3]\n",
      "[2 3 4]\n",
      "[3 4 5]\n",
      "[4 5 6]\n",
      "[5 6 7]\n",
      "[6 7 8]\n",
      "[7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# get iputs in numpy format\n",
    "ds = rolling_ds.flat_map(lambda window: window.batch(5)) # 5 - max single batch length\n",
    "for w in ds:\n",
    "    print(w.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] 2\n",
      "[1 2] 3\n",
      "[2 3] 4\n",
      "[3 4] 5\n",
      "[4 5] 6\n",
      "[5 6] 7\n",
      "[6 7] 8\n",
      "[7 8] 9\n"
     ]
    }
   ],
   "source": [
    "# lableling - split time window into x,y \n",
    "ds = ds.map(lambda window: (window[:-1], window[-1]))\n",
    "for x,y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence bias is when the order of things can impact the selection of things. For example, if I were to ask you your favorite TV show, and listed \"Game of Thrones\", \"Killing Eve\", \"Travellers\" and \"Doctor Who\" in that order, you're probably more likely to select 'Game of Thrones' as you are familiar with it, and it's the first thing you see. Even if it is equal to the other TV shows. So, when training data in a dataset, we don't want the sequence to impact the training in a similar way, so it's good to shuffle them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6] 7\n",
      "[1 2] 3\n",
      "[7 8] 9\n",
      "[3 4] 5\n",
      "[0 1] 2\n",
      "[6 7] 8\n",
      "[4 5] 6\n",
      "[2 3] 4\n"
     ]
    }
   ],
   "source": [
    "# shuffle to avoid sequence order bias at training.\n",
    "# buffer_size - number of previos elements to keep for random shuffle. Max distance between two shuffled items.\n",
    "ds = ds.shuffle(buffer_size=10)\n",
    "for x,y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [[6, 7], [0, 1]]\n",
      "y= [8 2]\n",
      "x= [[4, 5], [3, 4]]\n",
      "y= [6 5]\n",
      "x= [[7, 8], [5, 6]]\n",
      "y= [9 7]\n",
      "x= [[2, 3], [1, 2]]\n",
      "y= [4 3]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.batch(2).prefetch(1)\n",
    "for x,y in ds:\n",
    "    print(\"x=\", x.numpy().tolist())\n",
    "    print(\"y=\", y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
