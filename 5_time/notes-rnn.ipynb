{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Time Series\n",
    "\n",
    "Conceptualy very simple.\n",
    "![rnn-ts-concept.jpg](rnn-ts-concept.jpg)\n",
    "![rnn-dims.jpg](rnn-dims.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, Special case of RNN when you just using final prediction value - state to vector. It is same RNN with all unused outputs been ignored. It is default mode in TF. If want all Ys when **stack RNN layers** => set `return_sequences=True`\n",
    "\n",
    "![rnn-sq2vec.jpg](rnn-sq2vec.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, None, 20)          440       \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn-example1.jpg](rnn-example1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence RNN architecture, reuses same one Dense layer on all Ys. There are NO independed copies?\n",
    "![](rnn-sq2sq.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda layers\n",
    "Design to build in pre and post processing in NN and avoid transformation operations on data set => simplify code readability.\n",
    "![](rnn-lambda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very practical to use special callback **LearningRateScheduler** to train RNN on small number of epoches to choose lr that is most effective. \n",
    "\n",
    "Note **special loss function** (error metric) [Huber](https://en.wikipedia.org/wiki/Huber_loss) - it is less sensitive to the random noise, very handy for time series.\n",
    "\n",
    "![](rnn-lr.jpg)\n",
    "\n",
    "If run for 100 epoches we can see the most effective lr = (10e-5,10e-6)\n",
    "\n",
    "![](rnn-lr-choice.jpg)\n",
    "\n",
    "actual training is run with optimal lr.\n",
    "![](rnn-lr-optimal.jpg)\n",
    "![](rnn-overfitting.jpg)\n",
    "\n",
    "stopping earlier = no overfitting\n",
    "\n",
    "![](rnn-stop-earlier.jpg)\n",
    "\n",
    "Original notebook [S+P Week 3 Lesson 2 - RNN.ipynb](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%203%20Lesson%202%20-%20RNN.ipynb), local copy [rnn-ts-demo.ipynb](rnn-ts-demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Good to learn long sequences, mitigate gradient vaniting.\n",
    "[LSTM in pictures by Ng on Coursera](https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay)\n",
    "\n",
    "Original paper on LSTN\n",
    "\n",
    "![](lstm-origin.jpg)\n",
    "\n",
    "coding single (but bi-directional :) layer LSTN.\n",
    "![](lstn-coding.jpg)\n",
    "\n",
    "LSTN exceeds accuracy of two layers RNN.\n",
    "\n",
    "![](lstn-vs-rnn.jpg)\n",
    "![](lstn-two.jpg)\n",
    "\n",
    "Original notebook [S+P Week 3 Lesson 4 - LSTM.ipynb](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%203%20Lesson%204%20-%20LSTM.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
